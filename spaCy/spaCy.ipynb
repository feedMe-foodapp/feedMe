{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Tutorial of spaCy\n",
    "\n",
    "#### Documents, Spans & Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blank nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# create document\n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 1: Hello\n",
      "Token 2: World\n",
      "Token 3: !\n"
     ]
    }
   ],
   "source": [
    "# each token can be accessed by using the index\n",
    "token1 = doc[0]\n",
    "token2 = doc[1]\n",
    "token3 = doc[2]\n",
    "\n",
    "print(f'Token 1: {token1}')\n",
    "print(f'Token 2: {token2}')\n",
    "print(f'Token 3: {token3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "# a span is a slice from the object\n",
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Attributes\n",
    "\n",
    "With *is_alpha*, *is_punct* and *like_num*, it is possible to indicate whether the token consists of *alphabetic characters*, *number* or *punctuations*. These attributes are also called *lexical attributes* and refer to the entry in the vocabulary and don't depend on the context of the token itself. Therefore, it's easy to distinguish between alphabetic characters, numbers and punctuations. \n",
    "\n",
    "These flags simply return a boolean value and are stored in an array, like the following examples demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document\n",
    "doc = nlp(\"It costs $5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# get each index of document\n",
    "index = []\n",
    "\n",
    "for token in doc:\n",
    "    index.append(token.i)\n",
    "\n",
    "print(f'Index: {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n"
     ]
    }
   ],
   "source": [
    "# get each character of document\n",
    "text = []\n",
    "\n",
    "for token in doc:\n",
    "    text.append(token.text)\n",
    "\n",
    "print(f'Text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is alphabetic\n",
    "is_alpha = []\n",
    "\n",
    "for token in doc:\n",
    "    is_alpha.append(token.is_alpha)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'is_alpha: {is_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is numeric\n",
    "like_num = []\n",
    "\n",
    "for token in doc:\n",
    "    like_num.append(token.like_num)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'like_num: {like_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_punct: [False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is a punctuation\n",
    "is_punct = []\n",
    "\n",
    "for token in doc:\n",
    "    is_punct.append(token.is_punct)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'is_punct: {is_punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained Pipeline\n",
    "\n",
    "spaCy provides a number of trained pipeline packages. For example, the *en_core_web_sm* is a small English pipeline that supports all core cababilities and is trained especially on web-based text. The package provides the *binary weights* that enables spaCy to make predictions. It also includes the *vocabulary*, *meta information* and the *configuration file* used to train it. It tells spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each token in the document, it is possible to print out the text and the .pos_ attribute, \n",
    "the predicted part-of-speech tag. In spaCy, attributes that return strings usually end with an underscore - attributes without the underscore\n",
    "return an integer ID value.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d47b3602615d6995439a8c2d5578625bfd6a836233fcbfb77d2359be29ea918"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
