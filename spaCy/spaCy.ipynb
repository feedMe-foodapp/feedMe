{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Tutorial of spaCy\n",
    "\n",
    "#### Documents, Spans & Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blank nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# create document\n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 1: Hello\n",
      "Token 2: World\n",
      "Token 3: !\n"
     ]
    }
   ],
   "source": [
    "# each token can be accessed by using the index\n",
    "token1 = doc[0]\n",
    "token2 = doc[1]\n",
    "token3 = doc[2]\n",
    "\n",
    "print(f'Token 1: {token1}')\n",
    "print(f'Token 2: {token2}')\n",
    "print(f'Token 3: {token3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "# a span is a slice from the object\n",
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Attributes\n",
    "\n",
    "With *is_alpha*, *is_punct* and *like_num*, it is possible to indicate whether the token consists of *alphabetic characters*, *number* or *punctuations*. These attributes are also called *lexical attributes* and refer to the entry in the vocabulary and don't depend on the context of the token itself. Therefore, it's easy to distinguish between alphabetic characters, numbers and punctuations. \n",
    "\n",
    "These flags simply return a boolean value and are stored in an array, like the following examples demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document\n",
    "doc = nlp(\"It costs $5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# get each index of document\n",
    "index = []\n",
    "\n",
    "for token in doc:\n",
    "    index.append(token.i)\n",
    "\n",
    "print(f'Index: {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n"
     ]
    }
   ],
   "source": [
    "# get each character of document\n",
    "text = []\n",
    "\n",
    "for token in doc:\n",
    "    text.append(token.text)\n",
    "\n",
    "print(f'Text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is alphabetic\n",
    "is_alpha = []\n",
    "\n",
    "for token in doc:\n",
    "    is_alpha.append(token.is_alpha)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'is_alpha: {is_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is numeric\n",
    "like_num = []\n",
    "\n",
    "for token in doc:\n",
    "    like_num.append(token.like_num)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'like_num: {like_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_punct: [False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# check which character in the document is a punctuation\n",
    "is_punct = []\n",
    "\n",
    "for token in doc:\n",
    "    is_punct.append(token.is_punct)\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'is_punct: {is_punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained Pipeline\n",
    "\n",
    "spaCy provides a number of trained pipeline packages. For example, the *en_core_web_sm* is a small English pipeline that supports all core cababilities and is trained especially on web-based text. The package provides the *binary weights* that enables spaCy to make predictions. It also includes the *vocabulary*, *meta information* and the *configuration file* used to train it. It tells spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each token in the document, it is possible to print out the text and the .pos_ attribute, \n",
    "the predicted part-of-speech tag. In spaCy, attributes that return strings usually end with an underscore - attributes without the underscore\n",
    "return an integer ID value.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In addition to part-of-speech tags, it is also possible to predict how the words are related. \n",
    "For example, whether a word is the subject of the sentence or an object. The .dep_ attribute returns the predicted dependency label.\n",
    "The .head_ attribute returns the syntactic head token. \n",
    "\"\"\"\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based matching\n",
    "\n",
    "#### Why not just use regular expressions?\n",
    "\n",
    "Compared to regular expressions, the matcher works with *Doc* and *Token* objects instead of only strings. \n",
    "It's also more flexible and makes it possible to search for texts but also other lexical attributes and even write rules that uses a model's predictions.\n",
    "For example, find the word *duck* only if it is a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match patterns**\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "In this example, we're looking for two tokens with the text *iPhone* and *X*:\n",
    "\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal *iphone* and *x*:\n",
    "\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "It is also possible to write patterns using attributes predicted by a model. Here, we're matching a token with the lemma *buy*, plus a noun. \n",
    "The lemma is the base form, so this pattern would match phrases like *buying milk* or *bought flowers*:\n",
    "\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "Now it's time to use and try out the Matcher!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the Matcher from *spacy.matcher*. The matcher is initialized with the shared vocabulary *nlp.vocab* (more on that later).\n",
    "With *matcher.add*, the defined patterns are added to the object. The first argument is a unique ID to identify which pattern was matched and the second argument \n",
    "is a list of patterns. \n",
    "\n",
    "To match the pattern on a text, simply call the matcher on the doc. This will return the matches, trough we can iterate to print out the matching words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Loading the pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {\n",
    "        \"TEXT\": \"iPhone\"\n",
    "    },\n",
    "    {\n",
    "        \"TEXT\": \"X\"\n",
    "    }\n",
    "]\n",
    "\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process example text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "\"\"\"\n",
    "Iterate through the matches:\n",
    "- match_id: hash value of the pattern name\n",
    "- start: start index of matched span\n",
    "- end: end index of matched span\n",
    "\"\"\"\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "More complex pattern\n",
    "\n",
    "We are looking for five tokens:\n",
    "- A token consisting of only digits\n",
    "- Three case-insensitive tokens for 'fifa', 'world' & 'cup'\n",
    "- A token that consists of punctuation\n",
    "\"\"\"\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d47b3602615d6995439a8c2d5578625bfd6a836233fcbfb77d2359be29ea918"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
